{
  "filename": "client_rpc.md",
  "__html": "<h1>Definition of Apache TubeMQ RPC</h1>\n<h2>General Introduction</h2>\n<p>Implements of this part can be found in <code>org.apache.tubemq.corerpc</code>. Each node in Apache TubeMQ Cluster Communicates by TCP Keep-Alive. Mseeages are definded using binary and protobuf combined.\n<img src=\"img/client_rpc/rpc_bytes_def.png\" alt=\"\"></p>\n<p>All we can see in TCP are binary streams. We defind a 4-byte msgToken message <code>RPC\\_PROTOCOL\\_BEGIN\\_TOKEN</code> in header, which are used to distinguish each message and identify the legitimacy of the counterpart. When message client received is not started with these header field, client needs to close the connection and prompt the error and quit or reconnect because the protocal is not supported by TubeMQ or something wrong may happended. Follows is a 4-byte serialNo, this field is sent by client to server and returned by server exactly the same when after handling the request. It is mainly used to associate the context of the client request and response. And a 4-byte <code>listSize</code> field shows the number of data blocks in pb next, precisely the number of following <code>\\&amp;lt;len\\&amp;gt;\\&amp;lt;data\\&amp;gt;</code> blocks. This field would not be 0 in current definition. <code>\\&amp;lt;len\\&amp;gt;\\&amp;lt;data\\&amp;gt;</code> field is a combination of 2 filelds. That is, a length number and data, which mainly represents the length of this data block and the specific data.</p>\n<p>We defined <code>listSize</code> as <code>\\&amp;lt;len\\&amp;gt;\\&amp;lt;data\\&amp;gt;</code> because serialized PB data is saved as a ByteBuffer object in TubeMQ, and in Java, there a maximum(8196) length of ByteBuffer block, an overlength PB message needs to be saved in several ByteBuffer. No total length was counted, and the ByteBuffer is directly written when Serializing in to TCP message.</p>\n<p><strong>Pay more attention when implementing multiple languages and SDKs.</strong> Need to serialize PB data content into arrays of blocks(supported in PB codecs).</p>\n<h2>PB format code:</h2>\n<p>PB format encoding is divided into RPC framework definition, to the Master message encoding and to the Broker message encoding of three parts, you can use protobuf directly compiled to get different language codecs, it is very convenient to use.\n<img src=\"img/client_rpc/rpc_proto_def.png\" alt=\"\"></p>\n<p><code>RPC.proto</code> defines 6 struct, which divided into 2 class: Request message and Response message. Response message is divided into Successful Response and Exception Response.\n<img src=\"img/client_rpc/rpc_pbmsg_structure.png\" alt=\"\"></p>\n<p>The request message encoding and response message decoding can be implemented in the <code>NettyClient.java</code> class. There is some room for improvement in this part of the definition and can be found in <a href=\"https://issues.apache.org/jira/browse/TUBEMQ-109\">TUBEMQ-109</a>. However, due to compatibility concerns, it will be gradually replaced. We have implemented the current protobuf version, which is not a problem until at least 1.0.0. With the new protocol, the protocol implementation module requires each SDK to allow room for improvement. Take request message as an example, <code>RpcConnHeader</code> and other related structures are as follows：\n<img src=\"img/client_rpc/rpc_conn_detail.png\" alt=\"\"></p>\n<p>Flag marks whether the message is requested or not, and the next three marks represent the content of the message trace, which is not currently used; the related is a fixed mapping of the service type, protocol version, service type, etc., the more critical parameter RequestBody.timeout is the maximum allowable time from when a request is received by the server to when it is actually processed. Long wait time, discarded if exceeded, current default is 10 seconds, request filled as follows.\n<img src=\"img/client_rpc/rpc_header_fill.png\" alt=\"\"></p>\n<h2>Interactive diagram of the client's PB request &amp; response：</h2>\n<p><strong>Producer Interaction</strong>：</p>\n<p>The Producer has four pairs of instructions in the system, registration to master, heartbeat to master, exit from master and sending message to brokers.\n<img src=\"img/client_rpc/rpc_producer_diagram.png\" alt=\"\"></p>\n<p>Here we can see, Producer's implementation logic is to get metadata such as the list of partitions of specified topic from master, then select a partition and send message via TCP connection according to the rules of the client. It may be unsafe to send message without registration to master, the initial consideration was to use internal intake messages as much as possible and after that, considering security issues, we added authorization information carrying on top of this to perform authentication and authorization checks on the server side, solving the situation where the client bypasses the direct connection to the master and sends messages without authorization. But this will only enable in production environment.</p>\n<p><strong>Note in producer side of multiple languages implementation:</strong></p>\n<ol>\n<li>\n<p>Our Master is running as a hot-swap master, and the switchover is based on the information carried by the <code>RspExceptionBody</code>. In this case, you need to search for the keywords <code>&amp;quot;StandbyException&amp;quot;</code>, If this type of exception occurs, switch to another Master node for re-registration. This part has some relevant issues to adjust to the problem.</p>\n</li>\n<li>\n<p>Producer should re-register in the event of a Master connection failure during production, e.g. timeout, passive connection break, etc.</p>\n</li>\n<li>\n<p>Producer side should pay attention to the Broker pre-connection operation in advance: the back-end cluster can have hundreds of Broker nodes, and each Broker has about  ten partitions, so there will be thousands of possible records about the partition, after the SDK receives the metadata information from the Master, it should perform the connection establishment operation on the Broker that has not yet built the chain in advance.</p>\n</li>\n<li>\n<p>The Producer to Broker connection should be aware of anomaly detection and should be able to detect Broker bad spots and long periods of no messages, and to recycle the connection to Broker to avoid unstable operation in long-term running scene.</p>\n</li>\n</ol>\n<p><strong>Consumer Interaction Diagram</strong>:</p>\n<p>Consumer has 7 pairs of command in all, Register, Heartbeat, Exit to Master; Register, Logout, Heartbeat, Pulling mseeage to Broker. Registration and Logout to Broker is the same command, indicated by a different status code.</p>\n<p><img src=\"img/client_rpc/rpc_consumer_diagram.png\" alt=\"\"></p>\n<p>As we can see from the above picture, the Consumer first has to register to the Master, but registering to the Master can not get Metadata information immediately because TubeMQ is using a server-side load-balancing model, and the client needs to wait for the server to dispatch the consumption partition information; Consumer to Broker needs to register the logout operation. Partition is exclusive at the time of consumption, i.e., the same partition can only be consumed by one consumer in the same group at the same time. To solve this problem, the client needs to register and get consumption access to the partition; message pull and consumption confirmation need to appear in pairs. Although the protocol supports multiple pulls and then the last acknowledgement process, it is possible that the consumer permissions of a partition may be lost timeout from the client, thus This causes the data rollback to be triggered by repetitive consumption, and the more data is saved the more repetitive consumption will occur, so follow the 1:1 submission comparison fit.</p>\n<p>##Client feature:</p>\n<table>\n<thead>\n<tr>\n<th><strong>FEATURE</strong></th>\n<th><strong>Java</strong></th>\n<th><strong>C/C++</strong></th>\n<th><strong>Go</strong></th>\n<th><strong>Python</strong></th>\n<th><strong>Rust</strong></th>\n<th><strong>NOTE</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>TLS</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Authorization</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Anti-bypass-master production/consumption</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Distributed system with clients accessing Broker without Master's authentication authorization</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Effectively-Once</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Partition offset consumption</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Multiple Topic Consumption for a single Consumer group</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Server Consumption filter</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Auto shielding inactive Nodes</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Auto shielding bad Brokers</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Auto reconnect</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Auto recycling of Idle Connection</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Inactive for more than a specified period(e.g. 3min, mainly the producer side)</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Connection reuse</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Connection sharing according to the sessionFactory</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Unconnection reuse</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Asynchrounous Production</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Synchrounous Production</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Pull Consumption</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Push Consumption</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Consumption limit (QOS)</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Limit the amount of data per unit of time consumed by consumers</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Pull Consumption frequency limit</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Consumer Pull Consumption frequency limit</td>\n<td>✅</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2>Client function Induction CaseByCase：</h2>\n<p><strong>Client side and server side RPC interaction process</strong>：</p>\n<hr>\n<p><img src=\"img/client_rpc/rpc_inner_structure.png\" alt=\"\"></p>\n<p>As shown above, the client has to maintain local preservation of the sent request message until the RPC times out, or a response message is received and the response The message is associated by the SerialNo generated when the request is sent; the Broker information received from the server side, and the Topic information, which the SDK stores locally and updates with the latest returned information, as well as periodic reports to the Server side; the SDK is maintained to the heartbeat of the Master or Broker, and if Master feedback is found When the registration timeout error, re-registration operation should be carried out; SDK should be based on Broker connection establishment, the same process different Between objects, to allow the business to choose whether to support per-object or per-process connections.</p>\n<h2><strong>Message: Producer register to Master</strong>:</h2>\n<p><img src=\"img/client_rpc/rpc_producer_register2M.png\" alt=\"\"></p>\n<p><strong>ClientId</strong>：Producer needs to construct a ClientId at startup, and the current construction rule is:</p>\n<p>Java: ClientId = IPV4 + <code>&amp;quot;-&amp;quot;</code> + Thread ID + <code>&amp;quot;-&amp;quot;</code> + createTime + <code>&amp;quot;-&amp;quot;</code> + Instance ID + <code>&amp;quot;-&amp;quot;</code> + Client Version ID [+ <code>&amp;quot;-&amp;quot;</code> + SDK]. it is recommended that other languages add the above markup for easier access to the issue Exclusion. The ID value is valid for the lifetime of the Producer.</p>\n<p><strong>TopicList</strong>: The list of topics published by the user, Producer provides the initial list of topics for the data to be published at initialization, and also allows the business to defer adding new topics via the publish function in runtime, but does not support reducing topics in runtime.</p>\n<p><strong>brokerCheckSum</strong>: The check value of the Broker metadata information stored locally by the client, which is not available locally in Producer at initial startup, takes the value as -1; the SDK needs to carry the last BrokerCheckSum value on each request, and the Master determines whether the client's metadata needs to be updated by comparing the value.</p>\n<p><strong>hostname</strong>: The IPV4 address value of the machine where the Producer is located.</p>\n<p><strong>success</strong>: Whether the operation is successful, success is true, failure is false.</p>\n<p><strong>errCode</strong>: The code of error, currently one error code represents a large class of error, the specific cause of the error needs to be specifically identified by <code>errMsg</code>.</p>\n<p><strong>errMsg</strong>: The specific error message that the SDK needs to print out if something goes wrong.</p>\n<p><strong>authInfo</strong>：Authentication authorization information, if the user configuration is filled in to start authentication processing, then fill in; if authentication is required, then report according to the signature of the user name and password; if it is running, such as heartbeat, if the Master forces authentication processing, then report according to the signature of the user name and password; if not, then authenticate according to the authorization Token provided by the Master during the previous interaction; this authorization Token is also used to carry the message to Broker during production.</p>\n<p><strong>brokerInfos</strong>: Broker metadata information, which is primarily a list of Broker information for the entire cluster that the Master feeds back to the Producer in this field; the format is as follows.</p>\n<p><img src=\"img/client_rpc/rpc_broker_info.png\" alt=\"\"></p>\n<p><strong>authorizedInfo</strong>: Master provides authorization information in the following format.</p>\n<p><img src=\"img/client_rpc/rpc_master_authorizedinfo.png\" alt=\"\"></p>\n<p><strong>visitAuthorizedToken</strong>: To prevent clients from bypassing the Master's access authorization token, if that data is available, the SDK should save it locally and carry that information on subsequent visits to the Broker; if the field is changed on subsequent heartbeats, the locally cached data for that field needs to be updated.</p>\n<p><strong>authAuthorizedToken</strong>：Authenticated authorization tokens, if they have data for that field, they need to save and carry that field information for subsequent accesses to the Master and Broker; if the field is changed on subsequent heartbeats, the local cache of that field data needs to be updated.</p>\n<h2><strong>Mseeage: Heartbeat from Producer to Master</strong>:</h2>\n<p><img src=\"img/client_rpc/rpc_producer_heartbeat2M.png\" alt=\"\"></p>\n<p><strong>topicInfos</strong>: The metadata information corresponding to the Topic published by the SDK, including partition information and the Broker where it is located, is decoded. Since there is a lot of metadata, the outflow generated by passing the object data through as is would be very large, so we made Improvements.</p>\n<p><img src=\"img/client_rpc/rpc_convert_topicinfo.png\" alt=\"\"></p>\n<p><strong>requireAuth</strong>: Code to indicates the expiration of the previous authAuthorizedToken of the Master, requiring the SDK to report the username and password signatures on the next request.</p>\n<h2><strong>Message: Producer exits from Master</strong>:</h2>\n<p><img src=\"img/client_rpc/rpc_producer_close2M.png\" alt=\"\"></p>\n<p>Note that if authentication is enable, closing operation will do the authentication to avoid external interference with the operation.</p>\n<h2><strong>Message: Producer to Broker</strong>:</h2>\n<p>This part is related to the definition of RPC Message.</p>\n<p><img src=\"img/client_rpc/rpc_producer_sendmsg2B.png\" alt=\"\"></p>\n<p><strong>Data</strong> is the binary byte stream of Message.</p>\n<p><img src=\"img/client_rpc/rpc_message_data.png\" alt=\"\"></p>\n<p><strong>sentAddr</strong> is the local IPv4 address of the machine where the SDK is located converted to a 32-bit numeric ID.</p>\n<p><strong>msgType</strong> is the type of filter message. <code>msgTime</code> is the message time when the SDK sends a message, its value comes from the value filled in by <code>putSystemHeader</code> when constructing Message, and there is a corresponding API in Message to get it.</p>\n<p><strong>requireAuth</strong>: Required authentication operations to Broker for data production, not currently in effect due to performance issues. The authAuthorizedToken value in the sent message is based on the value provided by the Master and will change with the change of the Master.</p>\n<h2><strong>Partition Loadbalance</strong>:</h2>\n<p>Apache TubeMQ currently uses a server-side load balancing mode, where the balancing process is managed and maintained by the server; subsequent versions will add a client-side load balancing mode, so that two modes can co-exist.</p>\n<p><strong>Server side load balancing</strong>:</p>\n<ul>\n<li>When the Master process starts, it starts the load-balancing thread balancerChore. balancerChore periodically checks the current registered consumer group for load balancing. The process is simply to evenly distribute the consumer group subscription partitions to registered clients, and periodically detect the current partition of the client If so, the extra partitions will be split to other clients with less number of subscriptions. First, the master checks if the current consumer group needs load balancing. The topic collection is sorted by all partitions of the topic, and all consumer IDs of this consumer group, and then by the consumer group's all Divide and model the number of partitions and the number of clients to get the number of partitions each client subscribes to at most; then give each client the Assign partitions and carry the partition information in the heartbeat response when the consumer subscribes; if the client has more than one partition currently in place Give the client a partition release command to partition the partition away from the consumer, and to the assigned consumer A partition assignment instruction that informs that the partition is assigned to the corresponding client is as follows.</li>\n</ul>\n<p>Translated with <a href=\"http://www.DeepL.com/Translator\">www.DeepL.com/Translator</a> (free version)</p>\n<p><img src=\"img/client_rpc/rpc_event_proto.png\" alt=\"\"></p>\n<p><strong>rebalanceId</strong>：A long-type auto-increment number that indicates the round of load balance.</p>\n<p><strong>opType</strong>：Operation code, and its value defined in EventType. There are only four parts of the opcode that have been implemented, as follows: <code>DISCONNECT</code>, <code>CONNECT</code>, <code>REPORT</code> and <code>ONLY_</code>. Opcode started with <code>ONLY</code> is not detailed developed.</p>\n<p><img src=\"img/client_rpc/rpc_event_proto_optype.png\" alt=\"\"></p>\n<p><strong>status</strong>：Defined in <code>EventStatus</code>, indicates the status of the event. When Master constructs a load balancing task, it sets the status to <code>TODO</code>. When receiving the client heartbeat request, master writes the task to the response message and sets the status to <code>PROCESSING</code>. The client receives a load balancing command from the heartbeat response, and then it can perform the actual connection or disconnection operation, after the operation is finished, set the command status to <code>DONE</code> until sending next heartbeat to master.</p>\n<p><img src=\"img/client_rpc/rpc_event_proto_status.png\" alt=\"\"></p>\n<p><strong>subscribeInfo</strong> indicates assigned partition information, in the format suggested by the comment.</p>\n<ul>\n<li>Consumer Operation: When consumer receives metadata returned from master, it should establish the connection and release the operation(Refer to the opType note above). When connection established, return the operation result to master so that consumer can receive some relative job and perform. What we need to know is the LoadBalance of registration is a best-effort operation, if a new consumer send a request for connection before the consumer who occupanies the partition quits, it will receive <code>PARTITION\\_OCCUPIED</code> exception response. And at this time partition tries to remove it from its queue. And partition consumer will also remove it when receiving corresponding response so that the consumer could successfully register to this partition in next load balance.</li>\n</ul>\n",
  "link": "/en-us/docs/client_rpc.html",
  "meta": {
    "title": "Client RPC - Apache TubeMQ"
  }
}