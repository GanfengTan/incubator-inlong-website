{
  "filename": "deployment.md",
  "__html": "<h1>Compile, Deploy and Examples of TubeMQ ：</h1>\n<h2>Compile and Package Project：</h2>\n<p>Enter the root directory of project and run:</p>\n<pre><code>mvn clean package -Dmaven.test.skip\n</code></pre>\n<p>e.g. We put the TubeMQ project package at <code>E:/</code>, then run the above command. Compilation is complete when all subdirectories are compiled successfully.</p>\n<p><img src=\"img/sysdeployment/sys_compile.png\" alt=\"\"></p>\n<p>We can also run individual compilation in each subdirectory. Steps are the same as the whole project's compilation.</p>\n<p><strong>Server Deployment</strong></p>\n<p>As example above, entry directory <code>E:\\GIT\\TubeMQ\\tubemq-server\\target</code>, we can see several JARs. <code>tubemq-server-3.8.0-bin.tar.gz</code> is the complete server-side installation package， including execution scripts, configuration files, dependencies, and frontend source code. <code>tubemq-server-3.8.0.jar</code> is a server-side processing package included in <code>lib</code> of the complete project installer. Consider to daily changes and upgrades are most made to server side, we place this jar separately so that we just need to replace this jar during upgrade.</p>\n<p><img src=\"img/sysdeployment/sys_package.png\" alt=\"\"></p>\n<p>Here we have a complete package deployed onto server and we place it in <code>/data/tubemq</code></p>\n<p><img src=\"img/sysdeployment/sys_package_list.png\" alt=\"\"></p>\n<p><strong>Configuration System</strong></p>\n<p>There are 3 roles in server package: Master, Broker and Tools. Master and Broker can be deployed on the same or different machine. It depends on the bussiness layouts. As example below, we have 3 machine to startup a complete production and consumption cluster with 2 Masters.</p>\n<table>\n<thead>\n<tr>\n<th>Machine</th>\n<th>Role</th>\n<th>TCP Port</th>\n<th>TLS Port</th>\n<th>WEB Port</th>\n<th>Note</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10.224.148.145</td>\n<td><strong>Master</strong></td>\n<td>8099</td>\n<td>8199</td>\n<td>8080</td>\n<td>Metadata stored at <code>/stage/metadata</code></td>\n</tr>\n<tr>\n<td></td>\n<td>Broker</td>\n<td>8123</td>\n<td>8124</td>\n<td>8081</td>\n<td>Message stored at<code>/stage/msgdata</code></td>\n</tr>\n<tr>\n<td></td>\n<td>ZK</td>\n<td>2181</td>\n<td></td>\n<td></td>\n<td>Offset stored at root directory<code>/tubemq</code></td>\n</tr>\n<tr>\n<td>100.115.158.208</td>\n<td><strong>Master</strong></td>\n<td>8099</td>\n<td>8199</td>\n<td>8080</td>\n<td>Metadata stored at <code>/stage/metadata</code></td>\n</tr>\n<tr>\n<td></td>\n<td>Broker</td>\n<td>8123</td>\n<td>8124</td>\n<td>8081</td>\n<td>Message stored at<code>/stage/msgdata</code></td>\n</tr>\n<tr>\n<td>10.224.155.80</td>\n<td>Producer</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Consumer</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>Something should be noticed during deploying Master:</p>\n<ol>\n<li>Master cluster can be deployed in 1, 2 or 3 machines. 3 machines is suggested if HA is necessary so that reading/writing configuration and access to new production/consumption is still available when one of them is shutdown. In common situation, 2 machines provide readable configuration and proper state of production/consumption already registered when one is shutdown. The minimum is 1 and it provides proper state of production/consumption already registered when is shutdown.</li>\n<li>For machines with Master Role, we should promise clock synchronization. At the same time, IP address of each Master machine should be set in <code>/etc/hosts</code> on each Master machine.</li>\n</ol>\n<p><img src=\"img/sysdeployment/sys_address_host.png\" alt=\"\"></p>\n<p>Take <code>10.224.148.145</code> and <code>100.115.158.208</code> as examples, if we want to deploy both Master and Broker role on them, we need to configure in <code>/conf/master.ini</code>, <code>/resources/velocity.properties</code> and <code>/conf/broker.ini</code>. First set up the configuration of <code>10.224.148.145</code>,</p>\n<p><img src=\"img/sysdeployment/sys_configure_1.png\" alt=\"\"></p>\n<p>then it is <code>100.115.158.208</code>.</p>\n<p><img src=\"img/sysdeployment/sys_configure_2.png\" alt=\"\"></p>\n<p>Note that the upper right corner is configured with Master's web frontend configuration and configuration <code>file.resource.loader.path</code> in <code>/resources/velocity.properties</code> need to be modified according to the Master's installation path.</p>\n<p><strong>Start up Master</strong>：</p>\n<p>After configuration, entry directory <code>bin</code> of Master environment and start up master.</p>\n<p><img src=\"img/sysdeployment/sys_master_start.png\" alt=\"\"></p>\n<p>We First start up <code>10.224.148.145</code>, and then start up Master on <code>100.115.158.208</code>. The following messages indicate that the master and backup master have been successfully started up and the external service ports are reachable.</p>\n<p><img src=\"img/sysdeployment/sys_master_startted.png\" alt=\"\"></p>\n<p>Visiting Master's Administrator panel(<a href=\"http://100.115.158.208:8080/config/topic_list.htm\">http://100.115.158.208:8080/config/topic_list.htm</a>), search operation working well indicates that master has been successfully started up.</p>\n<p><img src=\"img/sysdeployment/sys_master_console.png\" alt=\"\"></p>\n<p><strong>Start up Broker</strong>：</p>\n<p>Starting up Broker is a little bit different to starting Master: Master is responsible for managing the entire TubeMQ cluster, including Broker node with Topic configuration on them, production and consumption managament. So we need to add metadata on Master before starting up Broker.</p>\n<p><img src=\"img/sysdeployment/sys_broker_configure.png\" alt=\"\"></p>\n<p>Confirm and create a draft record of Broker.</p>\n<p><img src=\"img/sysdeployment/sys_broker_online.png\" alt=\"\"></p>\n<p>We try to start up the Broker.</p>\n<p><img src=\"img/sysdeployment/sys_broker_start.png\" alt=\"\"></p>\n<p>But we got an error message.</p>\n<p><img src=\"img/sysdeployment/sys_broker_start_error.png\" alt=\"\"></p>\n<p>Because the broker record is currently in draft status and it is not available now. Let's go back to Master Administrator panel and publish.</p>\n<p><img src=\"img/sysdeployment/sys_broker_online_2.png\" alt=\"\"></p>\n<p>Every changing operation need to text in an Authorization Code when submited to Master. Authorization Code is defined by <code>confModAuthToken</code> in <code>master.ini</code>. If you have the Code of this cluster, we consider you as administrator and you have permission to operate the modification.</p>\n<p><img src=\"img/sysdeployment/sys_broker_deploy.png\" alt=\"\"></p>\n<p>Then we restart the Broker.</p>\n<p><img src=\"img/sysdeployment/sys_broker_restart_1.png\" alt=\"\"></p>\n<p><img src=\"img/sysdeployment/sys_broker_restart_2.png\" alt=\"\"></p>\n<p>Check the Master Control Panel, broker has successfully registered.</p>\n<p><img src=\"img/sysdeployment/sys_broker_finished.png\" alt=\"\"></p>\n<p><strong>Topic Configuration and Activation</strong>：</p>\n<p>Configuration of Topic is similar with Broker's, we should add metadata on Master before using them, otherwise it will report an Not Found Error during production/consumption. For example, if we try to consum a non-existent topic <code>test</code>,</p>\n<pre><code>/usr/local/java/default/bin/java -Xmx512m -Dlog4j.configuration=file:/data/tubemq/tubemq-server-3.8.0/conf/tools.log4j.properties -Djava.net.preferIPv4Stack=true -cp /data/tubemq/tubemq-server-3.8.0/lib/\\*:/data/tubemq/tubemq-server-3.8.0/conf/\\*: com.tencent.tubemq.example.MessageProducerExample 100.115.158.208 10.224.148.145:8000,100.115.158.208:8000 test 10000000 \n</code></pre>\n<p>Demo returns error message.</p>\n<p><img src=\"img/sysdeployment/sys_topic_error.png\" alt=\"\"></p>\n<p>First we add a topic in topic list page in Master Control Panel.</p>\n<p><img src=\"img/sysdeployment/sys_topic_create.png\" alt=\"\"></p>\n<p><img src=\"img/sysdeployment/sys_topic_select.png\" alt=\"\"></p>\n<p>Choose publish scope and confirm after submit topic detail. After adding a new topic, we need to overload the topic.</p>\n<p><img src=\"img/sysdeployment/sys_topic_deploy.png\" alt=\"\"></p>\n<p>Topic is available after overload. We can see some status of topic has changed after overload.</p>\n<p><img src=\"img/sysdeployment/sys_topic_finished.png\" alt=\"\"></p>\n<p><strong>Note</strong> When we are executing overload opertaion, we should make it in batches. Overload operations are controlled by state machines. It would become unwritable and un readale, read-only, readable and writable in order before published. Waiting for overloads on all brokers make topic temporary unreadable and unwritable, which result in production and consumption failure, especially production failure.</p>\n<p><strong>Message Production and Consumption</strong>：</p>\n<p>We pack Demo for test in package or <code>tubemq-client-3.8.0.jar</code> can be used for implementing your own production and consumption.\nWe run Producer Demo in below script and we can see data accepted on Broker.</p>\n<pre><code>/usr/local/java/default/bin/java -Xmx512m -Dlog4j.configuration=file:/data/tubemq/tubemq-server-3.8.0/conf/tools.log4j.properties -Djava.net.preferIPv4Stack=true -cp /data/tubemq/tubemq-server-3.8.0/lib/\\*:/data/tubemq/tubemq-server-3.8.0/conf/\\*: com.tencent.tubemq.example.MessageProducerExample 100.115.158.208 10.224.148.145:8000,100.115.158.208:8000 test 10000000 \n</code></pre>\n<p><img src=\"img/sysdeployment/sys_node_status.png\" alt=\"\"></p>\n<p>Then we run the Consumption Demo and we can see that consumption is also working properly.</p>\n<pre><code> /usr/local/java/default/bin/java -Xmx512m -Dlog4j.configuration=file:/data/tubemq/tubemq-server-3.8.0/conf/tools.log4j.properties -Djava.net.preferIPv4Stack=true -cp /data/tubemq/tubemq-server-3.8.0/lib/\\*:/data/tubemq/tubemq-server-3.8.0/conf/\\*: com.tencent.tubemq.example.MessageConsumerExample 10.224.148.145 10.224.148.145:8000,100.115.158.208:8000 test testGroup 3 1 1 \n\n</code></pre>\n<p><img src=\"img/sysdeployment/sys_node_status_2.png\" alt=\"\"></p>\n<p>As we can see, files relative to broker's production and consumption already exist.</p>\n<p><img src=\"img/sysdeployment/sys_node_log.png\" alt=\"\"></p>\n<p>Here, the compilation, deployment, system configuration, startup, production and consumption of TubeMQ has been completed!\nIf you need to get further, please refer to &quot;TubeMQ HTTP API&quot; and make your appropriate configuration settings.</p>\n",
  "link": "/en-us/docs/deployment.html",
  "meta": {
    "title": "Deployment - Apache TubeMQ"
  }
}